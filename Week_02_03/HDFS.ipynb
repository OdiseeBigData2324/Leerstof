{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HDFS\n",
    "\n",
    "HDFS is het distributed file system van Hadoop dat de basis vormt voor een breed gamma van applicaties, waaronder MapReduce.\n",
    "Dit framework maakt het mogelijk om niet-gespecialiseerde hardware te gebruiken om eenvoudig datacenters op te zetten of rekenclusters te beheren.\n",
    "HDFS bereikt dit doel op een aantal manieren:\n",
    "* Ten eerste wordt een file opgedeeld in verschillende blokken. Deze worden verdeeld over verschillende computers zodat code meerdere delen van het bestand kan gebruiken in parallel om zo de benodigde rekenkracht te verdelen over meerdere toestellen.\n",
    "* Daarnaast worden de blokken ook gedupliceerd om zo fout-toleranter te zijn in het geval van een crash (hardware of software), stroomstoring, netwerkonderbreking.\n",
    "\n",
    "Om dit te bereiken gebruikt Hadoop een Master-Slave architectuur dat bestaat uit een enkele namenode (master) en meerdere datanodes (slaves).\n",
    "De namenode houdt bij hoeveel datanodes er actief zijn, welke blokken ze hebben en welke blokken bij welke file horen.\n",
    "Indien er een datanode crasht gaat deze server dit detecteren en dit oplossen door de nodige blokken te kopieren van een andere datanode zodat er steeds voldoende kopies in het systeem aanwezig zijn.\n",
    "Bij elke actie die uitgevoerd moet worden in het HDFS moet er steeds gevraagd worden aan de namenode welke blokken op welke datanodes we nodig hebben voor de gewenste file uit te lezen of code voor uit te voeren.\n",
    "Het is dus duidelijk dat deze namenode een single-point-of-failure is wat ideaal is voor de availability van de cluster.\n",
    "Dit kan opgelost worden door HDFS te runnen in een high-availability mode wat ervoor zorgt dat er steeds een backup aanwezig is voor de namenode die zeer snel de werking kan overnemen.\n",
    "Deze structuur maakt het eenvoudig om aan horizontal scaling te doen door extra servers toe te voegen zonder dat er downtime is voor de hele cluster.\n",
    "\n",
    "Dit resulteer in de volgende kenmerken van HDFS:\n",
    "* High Throughput\n",
    "* Scalability\n",
    "* High Availability\n",
    "* Data Reliability\n",
    "* Fault Tolerance\n",
    "\n",
    "## Starten en stoppen van de cluster\n",
    "\n",
    "De cluster kan gestart worden door alle docker containers te starten die gedefinieerd worden in de yaml-file.\n",
    "Dit kan via de docker desktop gui of via de command line door middel van het docker-compose commando.\n",
    "Stoppen kan dan via dezelfde methodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Communiceren met het HDFS\n",
    "\n",
    "Er zijn verschillende manieren om dit distributed bestandssysteem te gebruiken.\n",
    "Ten eerste kan je gebruik maken van een command line interface (CLI) om bestanden uit te lezen, op te laden, ...\n",
    "Daarnaast zijn er ook wrappers voor verschillende talen die toelaten om rechtstreeks vanuit code te interageren met het HDFS.\n",
    "De voordelen van deze wrappers over een CLI zijn:\n",
    "* Flexibiler om te interageren in applicaties.\n",
    "* Gebruiksvriendelijker en gemakkelijker om te automatiseren.\n",
    "* Eenvoudiger in onderhoud en te debuggen\n",
    "* Volledige functionaliteit van een programmeertaal kan gebruikt worden zoals OO.\n",
    "\n",
    "### Instantiering van een client\n",
    "\n",
    "Veel verschillende talen beschikken over een wrapper om te communiceren met een HDFS.\n",
    "In deze cursus gaan we gebruik maken van [InsecureClient in hdfscli](https://hdfscli.readthedocs.io/en/latest/quickstart.html) wrapper.\n",
    "De eerste stap in het gebruik van de wrapper is te bepalen hoe de namenode van het hdfs gevonden kan worden.\n",
    "Dit gebeurt als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "client = InsecureClient(\"http://localhost:9870\", user='bigdata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# met status kan je gaan controleren of een file/folder bestaat\n",
    "client.status('/user/bigdata/HDFS/davinci_notebooks.txt', strict=False)\n",
    "fs_status = client.status('/user/bigdata/HDFS/davinci_notebooks2.txt', strict=False)\n",
    "if fs_status is None:\n",
    "    print('None')\n",
    "else:\n",
    "    print(fs_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Aanmaken van files en directories\n",
    "\n",
    "Om nu bestanden en folders aan te maken op dit distributed file systeem kunnen onderstaande functies gebruikt worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test = bestaat deze file = status\n",
    "!hdfs dfs -test -d /user/bigdata/HDFS/davinci_notebooks.txt\n",
    "!hdfs dfs -mkdir -p /user/bigdata/HDFS2\n",
    "\n",
    "# files aanmaken\n",
    "# put is schrijven\n",
    "!hadoop fs -put ulysses.txt /user/bigdata/HDFS2/cli.txt \n",
    "# get is lezen\n",
    "!hadoop fs -get /user/bigdata/HDFS2/cli.txt ulysses2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "HdfsError",
     "evalue": "Remote path '/user/bigdata/HDFS3/davinci_notebooks.txt' already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHdfsError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     client\u001b[38;5;241m.\u001b[39mmakedirs(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/bigdata/HDFS3\u001b[39m\u001b[38;5;124m'\u001b[39m)    \u001b[38;5;66;03m# /user/bigdata/HDFS3 is gelijk aan HDFS3 omdat we hierboven de user als bigdata ingesteld hebben\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# bestanden schrijven\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mHDFS3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdavinci_notebooks.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m client\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHDFS3/test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, read(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutline_of_science.txt\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/hdfs/client.py:599\u001b[0m, in \u001b[0;36mClient.upload\u001b[0;34m(self, hdfs_path, local_path, n_threads, temp_dir, chunk_size, progress, cleanup, **kwargs)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_name \u001b[38;5;129;01min\u001b[39;00m suffixes:\n\u001b[1;32m    598\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 599\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HdfsError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRemote path \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m already exists.\u001b[39m\u001b[38;5;124m'\u001b[39m, hdfs_path)\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    601\u001b[0m   temp_path \u001b[38;5;241m=\u001b[39m hdfs_path\n",
      "\u001b[0;31mHdfsError\u001b[0m: Remote path '/user/bigdata/HDFS3/davinci_notebooks.txt' already exists."
     ]
    }
   ],
   "source": [
    "# idem als hierboven maar in python code\n",
    "if client.status('/user/bigdata/HDFS3', strict=False) is None:\n",
    "    # als folder nog niet bestaat, maak hem dan aan\n",
    "    client.makedirs('/user/bigdata/HDFS3')    # /user/bigdata/HDFS3 is gelijk aan HDFS3 omdat we hierboven de user als bigdata ingesteld hebben\n",
    "\n",
    "# bestanden schrijven\n",
    "client.upload('HDFS3', 'davinci_notebooks.txt')\n",
    "\n",
    "client.write('HDFS3/test.txt', 'outline_of_science.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Bekijken van het filesysteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\n",
      "-rw-r--r--   3 root supergroup    1586336 2024-02-27 08:26 /user/bigdata/HDFS2/cli.txt\n"
     ]
    }
   ],
   "source": [
    "# met ! gaan we het onderstaande commando op de terminal uitvoeren\n",
    "# sommige versies van jupyter geven ze de voorkeur aan % ipv !\n",
    "!hdfs dfs -ls /user/bigdata/HDFS2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['davinci_notebooks.txt', 'test.txt']\n",
      "{'directoryCount': 0, 'ecPolicy': 'Replicated', 'fileCount': 1, 'length': 22, 'quota': -1, 'snapshotDirectoryCount': 0, 'snapshotFileCount': 0, 'snapshotLength': 0, 'snapshotSpaceConsumed': 0, 'spaceConsumed': 66, 'spaceQuota': -1, 'typeQuota': {}}\n",
      "{'entries': [], 'group': 'supergroup', 'owner': 'bigdata', 'permission': '644', 'stickyBit': False}\n",
      "{'algorithm': 'MD5-of-0MD5-of-512CRC32C', 'bytes': '0000020000000000000000009a18185c475e208de5db172f18473b2b00000000', 'length': 28}\n",
      "<generator object Client.walk at 0x7f0b51ee3220>\n",
      "('/user/bigdata', ['ETL', 'HDFS', 'HDFS2', 'HDFS3', 'MLLib', 'MapReduce', 'Oef_MapReduce', 'Oefeningen', 'Project', 'Spark'], [])\n",
      "('/user/bigdata/ETL', [], ['cars.csv'])\n",
      "('/user/bigdata/HDFS', [], ['davinci_notebooks.txt'])\n",
      "('/user/bigdata/HDFS2', [], ['cli.txt'])\n",
      "('/user/bigdata/HDFS3', [], ['davinci_notebooks.txt', 'test.txt'])\n",
      "('/user/bigdata/MLLib', ['chihuahua-or-muffin'], [])\n",
      "('/user/bigdata/MLLib/chihuahua-or-muffin', [], ['chihuahua-1.jpg', 'chihuahua-2.jpg', 'chihuahua-3.jpg', 'chihuahua-4.jpg', 'chihuahua-5.jpg', 'chihuahua-6.jpg', 'chihuahua-7.jpg', 'chihuahua-8.jpg', 'muffin-1.jpeg', 'muffin-2.jpeg', 'muffin-3.jpeg', 'muffin-4.jpeg', 'muffin-5.jpeg', 'muffin-6.jpeg', 'muffin-7.jpeg', 'muffin-8.jpeg'])\n",
      "('/user/bigdata/MapReduce', ['output', 'output.txt', 'output_java', 'output_mean_length'], ['input.txt'])\n",
      "('/user/bigdata/MapReduce/output', [], ['_SUCCESS', 'part-r-00000'])\n",
      "('/user/bigdata/MapReduce/output.txt', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/MapReduce/output_java', [], ['_SUCCESS', 'part-r-00000'])\n",
      "('/user/bigdata/MapReduce/output_mean_length', [], ['_SUCCESS', 'part-r-00000'])\n",
      "('/user/bigdata/Oef_MapReduce', [], ['games.csv'])\n",
      "('/user/bigdata/Oefeningen', ['Spark'], [])\n",
      "('/user/bigdata/Oefeningen/Spark', ['model'], ['song_data.csv'])\n",
      "('/user/bigdata/Oefeningen/Spark/model', ['bestModel', 'estimator', 'evaluator', 'metadata'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel', ['metadata', 'stages'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages', ['0_VectorAssembler_6db99c0d9560', '1_StandardScaler_1fd79c11999f', '2_RandomForestRegressor_d32f843532d4'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/0_VectorAssembler_6db99c0d9560', ['metadata'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/0_VectorAssembler_6db99c0d9560/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/1_StandardScaler_1fd79c11999f', ['data', 'metadata'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/1_StandardScaler_1fd79c11999f/data', [], ['_SUCCESS', 'part-00000-7b3d03ea-06fb-4756-aff4-488cbc622064-c000.snappy.parquet'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/1_StandardScaler_1fd79c11999f/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/2_RandomForestRegressor_d32f843532d4', ['data', 'metadata', 'treesMetadata'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/2_RandomForestRegressor_d32f843532d4/data', [], ['_SUCCESS', 'part-00000-23256184-b303-40d0-845a-586e7beadc83-c000.snappy.parquet'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/2_RandomForestRegressor_d32f843532d4/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/bestModel/stages/2_RandomForestRegressor_d32f843532d4/treesMetadata', [], ['_SUCCESS', 'part-00000-516ea59c-aa20-4e9d-acfc-1db6fc2b94d5-c000.snappy.parquet'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator', ['metadata', 'stages'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator/stages', ['0_VectorAssembler_6db99c0d9560', '1_StandardScaler_1fd79c11999f', '2_RandomForestRegressor_d32f843532d4'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator/stages/0_VectorAssembler_6db99c0d9560', ['metadata'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator/stages/0_VectorAssembler_6db99c0d9560/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator/stages/1_StandardScaler_1fd79c11999f', ['metadata'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator/stages/1_StandardScaler_1fd79c11999f/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator/stages/2_RandomForestRegressor_d32f843532d4', ['metadata'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/estimator/stages/2_RandomForestRegressor_d32f843532d4/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/evaluator', ['metadata'], [])\n",
      "('/user/bigdata/Oefeningen/Spark/model/evaluator/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Oefeningen/Spark/model/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project', ['output', 'project.model', 'tweets'], ['data.csv', 'tweets2188615777.txt'])\n",
      "('/user/bigdata/Project/output', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model', ['bestModel', 'estimator', 'evaluator', 'metadata'], [])\n",
      "('/user/bigdata/Project/project.model/bestModel', ['metadata', 'stages'], [])\n",
      "('/user/bigdata/Project/project.model/bestModel/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages', ['0_Tokenizer_a8a47cd6f355', '1_HashingTF_d5e9c3c80c15', '2_IDF_dc20da8e7d9c', '3_RandomForestClassifier_2537271054b5', '4_IndexToString_5fc2755c10b9'], [])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/0_Tokenizer_a8a47cd6f355', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/0_Tokenizer_a8a47cd6f355/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/1_HashingTF_d5e9c3c80c15', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/1_HashingTF_d5e9c3c80c15/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/2_IDF_dc20da8e7d9c', ['data', 'metadata'], [])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/2_IDF_dc20da8e7d9c/data', [], ['_SUCCESS', 'part-00000-91500554-561c-4bb3-ba1c-240c2e547877-c000.snappy.parquet'])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/2_IDF_dc20da8e7d9c/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/3_RandomForestClassifier_2537271054b5', ['data', 'metadata', 'treesMetadata'], [])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/3_RandomForestClassifier_2537271054b5/data', [], ['_SUCCESS', 'part-00000-343781c3-4479-45d7-a3bb-4b8b1062ad80-c000.snappy.parquet'])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/3_RandomForestClassifier_2537271054b5/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/3_RandomForestClassifier_2537271054b5/treesMetadata', [], ['_SUCCESS', 'part-00000-ed5b8300-1137-41a3-ac8b-a7bb8916f8dd-c000.snappy.parquet'])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/4_IndexToString_5fc2755c10b9', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/bestModel/stages/4_IndexToString_5fc2755c10b9/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/estimator', ['metadata', 'stages'], [])\n",
      "('/user/bigdata/Project/project.model/estimator/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/estimator/stages', ['0_Tokenizer_a8a47cd6f355', '1_HashingTF_d5e9c3c80c15', '2_IDF_dc20da8e7d9c', '3_RandomForestClassifier_2537271054b5', '4_IndexToString_5fc2755c10b9'], [])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/0_Tokenizer_a8a47cd6f355', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/0_Tokenizer_a8a47cd6f355/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/1_HashingTF_d5e9c3c80c15', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/1_HashingTF_d5e9c3c80c15/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/2_IDF_dc20da8e7d9c', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/2_IDF_dc20da8e7d9c/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/3_RandomForestClassifier_2537271054b5', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/3_RandomForestClassifier_2537271054b5/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/4_IndexToString_5fc2755c10b9', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/estimator/stages/4_IndexToString_5fc2755c10b9/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/evaluator', ['metadata'], [])\n",
      "('/user/bigdata/Project/project.model/evaluator/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/project.model/metadata', [], ['_SUCCESS', 'part-00000'])\n",
      "('/user/bigdata/Project/tweets', [], ['1973278273.txt', '1973278356.txt', '1973278363.txt', '1973278391.txt', '1973278397.txt', '1973278409.txt', '1973278418.txt', '1973278466.txt', '1973278605.txt', '1973278611.txt', '1973278617.txt', '1973278621.txt', '1973278656.txt', '1973278699.txt', '1973278749.txt', '1973278786.txt', '1973278840.txt', '1973278910.txt', '1973278927.txt', '1973279020.txt', '1973279068.txt', '1973279079.txt', '1973279092.txt', '1973279206.txt', '1973279218.txt', '1973279242.txt', '1973279255.txt', '1973279293.txt', '1973279346.txt', '1973279360.txt', '1973279453.txt', '1973279476.txt', '1973279487.txt', '1973279535.txt'])\n",
      "('/user/bigdata/Spark', [], ['titanic.csv'])\n"
     ]
    }
   ],
   "source": [
    "print(client.list('HDFS3'))\n",
    "print(client.content('HDFS3/test.txt'))\n",
    "# deze gaan wij minder gaan gebruiken\n",
    "print(client.acl_status('HDFS3/davinci_notebooks.txt'))\n",
    "print(client.checksum('HDFS3/davinci_notebooks.txt'))\n",
    "print(client.walk('/'))   # dit geeft een generator terug, dit kan je in een for-lus plaatsen\n",
    "for f in client.walk('/user/bigdata'):\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uitlezen van het filesysteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'outline_of_science.txt'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/bigdata/workspace2324/Week 02/notebooks2.txt'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with client.read('HDFS3/test.txt') as reader:\n",
    "    content = reader.read()\n",
    "    print(content)\n",
    "\n",
    "client.download('HDFS3/davinci_notebooks.txt', 'notebooks2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aanpassen van het filesysteem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "client.set_replication('HDFS3/test.txt', 1)\n",
    "client.set_permission('HDFS3/davinci_notebooks.txt', 700)\n",
    "\n",
    "client.rename('HDFS3/test.txt', 'HDFS3/test2.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.delete('/user/bigdata/HDFS3', recursive=True)\n",
    "client.delete('/user/bigdata/HDFS2', recursive=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Schrijf python code dat controleert of een directory bestaat in het hdfs.\n",
    "Indien nee wordt de directory aangemaakt.\n",
    "Indien ja, worden alle files in de directory verwijderd om van een lege directory te starten.\n",
    "Upload daarna een tekst-bestand naar keuze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient\n",
    "client = InsecureClient(\"http://localhost:9870\", user='bigdata')\n",
    "map = 'HDFS'\n",
    "input_files = [ 'davinci_notebooks.txt', 'outline_of_science.txt', 'ulysses.txt']\n",
    "\n",
    "if client.status(map, strict=False) is None:\n",
    "    client.makedirs(map)\n",
    "else:\n",
    "    for f in client.list(map):\n",
    "        # extra check om ervoor te zorgen dat startdata niet verwijderd wordt\n",
    "        if not f.endswith('.txt'):\n",
    "            client.delete(map + \"/\" + f, recursive=True)\n",
    "\n",
    "for f in input_files:\n",
    "    if client.status(map + \"/\" + f) is None:\n",
    "        client.upload(map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
