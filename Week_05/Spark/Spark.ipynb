{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Spark\n",
    "\n",
    "Hoewel het MapReduce algoritme van Hadoop een aantal voordelen heeft. \n",
    "De meest beperkende eigenschap van het MapReduce algoritme is de snelheid.\n",
    "Omdat alles ingelezen wordt vanaf de harde schijf, tussenresultaten op de schijf opgeslagen worden en de finale resultaten ook wordt er tot wel 90% van de rekentijd gespendeerd in lees- of schrijfopdrachten.\n",
    "\n",
    "Spark is geintroduceerd om dit te versnellen door gebruik te maken van in-memory processing.\n",
    "Hierdoor is Spark tot 3 keer sneller op grote datasets en tot 100 keer op kleinere datasets.\n",
    "\n",
    "Het spark framework kan gebruik maken van een externe opslag-locatie voor bestanden bij te houden (zoals HDFS) en bestaat uit de volgende componenten:\n",
    "* SparkCore\n",
    "* Spark SQL\n",
    "* Spark Streaming\n",
    "* MLlib\n",
    "* SparkGraph\n",
    "\n",
    "Daarnaast zijn er ook verschillende Spark Api's voor verschillende programmeertalen zoals Python, Scala, Java, ...\n",
    "Hierdoor is het framework ook flexibeler dan het standaard MapReduce algoritme.\n",
    "Heel veel informatie over het spark framework vind je in de [documentatie](https://spark.apache.org/docs/latest/quick-start.html) en de programming guides (bovenaan)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from hdfs import InsecureClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World,\n",
      "hello world,\n",
      "hello world,\n",
      "\n",
      "Dit is een voorbeeld file om het Wordcount voorbeeld te testen !\n"
     ]
    }
   ],
   "source": [
    "map = 'Spark'\n",
    "\n",
    "client = InsecureClient('http://localhost:9870', user='bigdata')\n",
    "\n",
    "if client.status(map, strict=False) is None:\n",
    "    client.makedirs(map)\n",
    "else:\n",
    "    # do some cleaning in case anything else than *.txt is present\n",
    "    for f in client.list(map):\n",
    "        client.delete(map + '/' + f, recursive=True)\n",
    "\n",
    "client.upload(map, 'input.txt')\n",
    "with client.read(map + '/input.txt') as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Installatie\n",
    "\n",
    "Een python implementatie van Spark kan eenvoudig geinstalleerd worden door het volgende commando uit te voeren. \n",
    "Dit moet maar eenmalig gebeuren.\n",
    "Om te kijken of het reeds geinstalleerd is kan je kijken naar de versie van pyspark (indien geinstalleerd). \n",
    "Als de versie correct gereturned wordt, dan is het reeds geinstalleerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /___/ .__/\\_,_/_/ /_/\\_\\   version 3.5.0\n",
      "      /_/\n",
      "                        \n",
      "Using Scala version 2.12.18, OpenJDK 64-Bit Server VM, 1.8.0_392\n",
      "Branch HEAD\n",
      "Compiled by user ubuntu on 2023-09-09T01:53:20Z\n",
      "Revision ce5ddad990373636e94071e7cef2f31021add07b\n",
      "Url https://github.com/apache/spark\n",
      "Type --help for more information.\n"
     ]
    }
   ],
   "source": [
    "!pyspark --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pyspark\n",
    "# dit moet je niet uitvoeren, is reeds geinstalleerd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark kan op drie manieren werken:\n",
    "* Boven op MapReduce (traag)\n",
    "* Boven op Yarn\n",
    "* Via zijn eigen resource manager\n",
    "\n",
    "In deze notebook gaan we gebruik maken van Spark gebruikmakende van yarn.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resilient Distributed Datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voor de configuratie moeten we vooral twee zaken aangeven, namelijk:\n",
    "* Naam van de applicatie (is zichtbaar in de yarn)\n",
    "* Master url. De url dat het type cluster en hoe het te bereiken aangeeft. Wij gaan vooral werken met local om te communiceren met het lokale bestandssysteem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/19 09:22:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Wordcount voorbeeld\n",
    "\n",
    "Om de api van pyspark te leren kennen kan je gaan naar de [documentatie](https://spark.apache.org/docs/latest/api/python/reference/index.html).\n",
    "Een eerder stap bij stap uitleg kan je [hier](https://spark.apache.org/docs/latest/api/python/getting_started/index.html) vinden.\n",
    "\n",
    "In onderstaande code gaan we stap voor stap het wordcount-voorbeeld uitwerken.\n",
    "\n",
    "Eerst moet er een pyspark context aangemaakt worden als volgt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bestand uitlezen\n",
    "df = spark.read.text('/user/bigdata/Spark/input.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|       value|\n",
      "+------------+\n",
      "|Hello World,|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.count()\n",
    "df.first()\n",
    "\n",
    "# filter op de rijen die Hello bevatten\n",
    "# show print het volledige resultaat\n",
    "df.filter(df.value.contains('Hello')).show()\n",
    "# collect brengt alle data lokaal van het gedistribueerde dataframe\n",
    "# collect gebruik je liefst zo weinig mogelijk om te grote datasets te downloaden te vermijden\n",
    "df.filter(df.value.contains('Hello')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|               value|contains|\n",
      "+--------------------+--------+\n",
      "|        Hello World,|    true|\n",
      "|        hello world,|   false|\n",
      "|        hello world,|   false|\n",
      "|                    |   false|\n",
      "|Dit is een voorbe...|   false|\n",
      "+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# met withColumn kan je kolommen voor bewerkingen gaan toevoegen\n",
    "df = df.withColumn('contains', df.value.contains('Hello'))\n",
    "df.show()\n",
    "\n",
    "# alternatief voor in pandas\n",
    "# df[\"contains\"] = df.value.contains('Hello')\n",
    "\n",
    "# immuatable -> onveranderbaar dus worden nieuwe object aangemaakt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Row(value='Hello\", 1),\n",
       " (\"World,')\", 1),\n",
       " (\"Row(value='hello\", 1),\n",
       " (\"world,')\", 1),\n",
       " (\"Row(value='hello\", 1),\n",
       " (\"world,')\", 1),\n",
       " (\"Row(value='')\", 1),\n",
       " (\"Row(value='Dit\", 1),\n",
       " ('is', 1),\n",
       " ('een', 1),\n",
       " ('voorbeeld', 1),\n",
       " ('file', 1),\n",
       " ('om', 1),\n",
       " ('het', 1),\n",
       " ('Wordcount', 1),\n",
       " ('voorbeeld', 1),\n",
       " ('te', 1),\n",
       " ('testen', 1),\n",
       " (\"!')\", 1)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.flatMap(lambda line: str(line).split(\" \")).map(lambda w: (w,1)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(\"Row(value='Hello\", 1),\n",
       " (\"World,')\", 1),\n",
       " (\"Row(value='hello\", 2),\n",
       " (\"world,')\", 2),\n",
       " (\"Row(value='')\", 1),\n",
       " (\"Row(value='Dit\", 1),\n",
       " ('is', 1),\n",
       " ('een', 1),\n",
       " ('voorbeeld', 2),\n",
       " ('file', 1),\n",
       " ('om', 1),\n",
       " ('het', 1),\n",
       " ('Wordcount', 1),\n",
       " ('te', 1),\n",
       " ('testen', 1),\n",
       " (\"!')\", 1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = df.rdd.flatMap(lambda line: str(line).split(' '))    # met .rdd zetten we een dataframe om naar zijn rdd\n",
    "# flatMap -> een soort map operatie\n",
    "# Hello World,     ->      ['Hello', 'World,']      (map operatie)\n",
    "\n",
    "#flatMap doet het volgende\n",
    "    # Hello World,\n",
    "# omzetten naar\n",
    "    # Hello\n",
    "    # World, \n",
    "words_mapped = words.map(lambda w: (w,1))            # map-functie om per entry een 1-waarde toe te voegen als value\n",
    "words_reduced = words_mapped.reduceByKey(lambda a,b: a+b)   # tel alle waarden per key op\n",
    "\n",
    "words_reduced.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wat gebeurt er in dit voorbeeld?**\n",
    "\n",
    "Sparkcontext om een connectie te maken met de distributed storage\n",
    "De input file wordt dan ingelezen met de textFile functie.\n",
    "Door middel van de flatMap functie wordt de tekst lijn per lijn ingelezen en gesplits in woorden. \n",
    "Dit resulteert in een RDD (Resilient Distributed Dataset.\n",
    "De .map() functie maakt een key-value pair aan voor elke keer dat het woord voorkomt.\n",
    "In een laatste fase is er een reduce stap per key die de som neemt van alle keren dat het woord voorkomt om de uiteindelijke wordcount te nemen.\n",
    "Of af te ronden wordt het resultaat opgeslagen.\n",
    "\n",
    "![spark wordcount in yarn](images/yarn_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSession\n",
    "\n",
    "Nu gaan we stuk voor stuk de verschillende stappen bekijken om een pyspark applicatie te maken.\n",
    "De eerste stap is het aanmaken van een sessie (SparkSession) wat het beginpunt is voor spark applications.\n",
    "Er zijn twee manieren om een SparkSession aan te maken:\n",
    "* builder()\n",
    "* newSession()\n",
    "\n",
    "Bij het aanmaken van een session wordt er intern een SparkContext object aangemaakt. \n",
    "Dit object stelt de connectie naar een cluster voor.\n",
    "Er kan maar 1 context tegelijkertijd actief zijn.\n",
    "Als je wil connecteren met een tweede cluster moet je eerst stop() oproepen op de reeds actieve context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 09:58:23 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor). This may indicate an error, since only one SparkContext should be running in this JVM (see SPARK-2243). The other SparkContext was created at:\n",
      "org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "py4j.Gateway.invoke(Gateway.java:238)\n",
      "py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "java.lang.Thread.run(Thread.java:750)\n",
      "24/03/19 09:58:23 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark.stop() # stop de huidige sparksession\n",
    "\n",
    "#.master(\"local[2]\")         # run lokaal met twee cores\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"Word Count\").getOrCreate()   # get or create -> singleton constructie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RDD\n",
    "\n",
    "Op basis van het SparkSession object is het dan mogelijk om RDD-objecten aan te maken.\n",
    "Een RDD is de basis dataobject binnen Spark dat in parallel op verschillende nodes binnen een cluster kan uitgevoerd worden.\n",
    "Alle dataobjecten binnen spark horen tot deze klassen en dus zijn er veel mogelijkheden om RDD's aan te maken.\n",
    "Hier haal ik er twee aan:\n",
    "* parallelize() om bestaande python objecten om te zetten naar een RDD\n",
    "* textFile() of andere read methoden om bestanden op de cluster uit te lezen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# voorbeeld van parallelize \n",
    "dataList = [(\"Student1\", 8), (\"Student2\", 16), (\"Student3\", 11)]\n",
    "rdd=spark.sparkContext.parallelize(dataList)\n",
    "\n",
    "# voorbeeld van inlezen van een file\n",
    "#rdd2 = spark.read.text(\"/user/bigdata/Spark/input.txt\")\n",
    "#rdd2 = spark.sparkContext.textFile(\"/user/bigdata/Spark/input.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Met bovenstaande methoden hebben we twee rdd's aangemaakt. \n",
    "Op deze objecten kunnen nu verscheidene operaties uitgevoerd worden.\n",
    "Een belangrijke eigenschap van dit type objecten is dat ze steeds in parallel uitgevoerd worden.\n",
    "\n",
    "De beschikbare operaties kunnen in twee groepen verdeeld worden:\n",
    "* transformaties\n",
    "* acties\n",
    "\n",
    "[Transformaties](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-transformations/) zijn lazy-operations waarvoor de berekening uitgesteld wordt en geven een nieuw RDD terug.\n",
    "Een aantal voorbeelden van transformaties zijn:\n",
    "* flatMap()\n",
    "* map()\n",
    "* reduceByKey()\n",
    "* filter()\n",
    "* sortByKey()\n",
    "\n",
    "[Acties](https://sparkbyexamples.com/apache-spark-rdd/spark-rdd-actions/) zijn operaties die een berekening starten (ook van de nodige transformaties) en geven een niet RDD-object terug. \n",
    "Een aantal voorbeelden hiervan zijn:\n",
    "* count()\n",
    "* collect()\n",
    "* first()\n",
    "* max()\n",
    "* reduce()\n",
    "\n",
    "Lees nu bovenstaande links en geef de functies die nodig zijn voor de volgende vragen op te lossen. Geef ook aan of het transformaties zijn of acties:\n",
    "* Het aantal keer dat elke waarde aanwezig is in de dataset (1 functie voor wordcount uit te voeren)\n",
    "* Uitfilteren van rijen\n",
    "* Groeperen van een aantal rijen op basis van een bepaalde waarde.\n",
    "* Toevoegen van een kolom aan elke key (bvb de lengte van een woord)\n",
    "* Hoe doe je head() uit pandas op RDD's?\n",
    "* Hoe doe je de apply() uit pandas op RDD's?\n",
    "\n",
    "Maak nu een spark applicatie dat van de eerste RDD (met de studenten) telt hoeveel studenten geslaagd zijn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.filter(lambda student: student[1] >=10).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De applicatie voor het berekenen van een gemiddelde is iets complexer.\n",
    "Dit soort applicaties kan geschreven worden als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.666666666666666"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aantal = rdd.count()\n",
    "\n",
    "rdd.map(lambda x: x[1]).reduce(lambda x,y: (x+y))/ aantal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schrijf nu een mapreduce applicatie in spark om de tweede RDD van de input te verwerken en het aantal woorden van elke lengte te bekomen.\n",
    "Tussenresultaten kan je tonen om te debuggen met de .collect() functie. Dit haalt de dataset uit het gedistribueerde Spark geheugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(16, 3), (8, 3), (13, 1), (14, 1), (2, 3), (3, 3), (9, 3), (4, 1), (6, 1)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = df.rdd.flatMap(lambda line: str(line).split(' '))   \n",
    "words_mapped = words.map(lambda w: (len(w),1))            \n",
    "words_reduced = words_mapped.reduceByKey(lambda a,b: a+b)   \n",
    "\n",
    "words_reduced.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframes\n",
    "\n",
    "Een belangrijke subklasse van RDD's zijn dataframes.\n",
    "Dit is een veel gebruikte manier om gestructureerde data voor te stellen.\n",
    "Dataframes in spark is sterk gerelateerd aan de dataframes gezien in pandas.\n",
    "Het belangrijskte verschil is dat ze verdeeld worden over de cluster en operaties op de dataframes in parallel uitgevoerd worden.\n",
    "Dataframes kunnen aangemaakt worden door gebruik te maken van de createDataFrame functie in context of ingelezen worden vanuit csv's of jsons. Ten slotte kunnen dataframes ook komen van externe bronnen zoals databases als resultaat van een sql-query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------+---------+\n",
      "|firstname|lastname|       dob|gender|   budget|\n",
      "+---------+--------+----------+------+---------+\n",
      "|    Harry|  Potter|1980-07-31|     M|100000000|\n",
      "|   Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "|Hermelijn| Griffel|1979-09-19|     F|     4000|\n",
      "+---------+--------+----------+------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/19 10:29:36 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 11:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------+----------+------+-------------------+\n",
      "|summary|firstname|lastname|       dob|gender|             budget|\n",
      "+-------+---------+--------+----------+------+-------------------+\n",
      "|  count|        3|       3|         3|     3|                  3|\n",
      "|   mean|     NULL|    NULL|      NULL|  NULL|         3.333467E7|\n",
      "| stddev|     NULL|    NULL|      NULL|  NULL|5.773386936614157E7|\n",
      "|    min|    Harry| Griffel|1979-09-19|     F|                 10|\n",
      "|    max|   Ronald|   Wemel|1980-07-31|     M|          100000000|\n",
      "+-------+---------+--------+----------+------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data = [('Harry', 'Potter','1980-07-31','M',100000000),\n",
    "  ('Ronald','Wemel','1980-04-01','M',10),\n",
    "  ('Hermelijn','Griffel','1979-09-19','F',4000)\n",
    "]\n",
    "\n",
    "columns = [\"firstname\",\"lastname\",\"dob\",\"gender\",\"budget\"]\n",
    "df = spark.createDataFrame(data=data, schema = columns)\n",
    "df.show()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PySpark SQL\n",
    "\n",
    "Bovenstaande datastructuren (RDD's en Dataframes) zijn een onderdeel van het Pyspark sql module.\n",
    "De Spark API heeft een hele reeks methoden en functies om deze in te laden, uit te lezen en te manipuleren.\n",
    "Daarnaast maakt deze module het ook mogelijk om SQL-queries uit te voeren op dataframes.\n",
    "Om SQL-queries uit te voeren op dataframes moet er eerst een view gemaakt worden in het dataframe met de functie createOrReplaceTempView(\"view_name\")\n",
    "\n",
    "Daarna kan je gebruik maken van de .sql() functie om allerhande sql queries uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+----------+------+---------+\n",
      "|firstname|lastname|       dob|gender|   budget|\n",
      "+---------+--------+----------+------+---------+\n",
      "|    Harry|  Potter|1980-07-31|     M|100000000|\n",
      "|   Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "+---------+--------+----------+------+---------+\n",
      "\n",
      "+---------+--------+----------+------+---------+\n",
      "|firstname|lastname|       dob|gender|   budget|\n",
      "+---------+--------+----------+------+---------+\n",
      "|    Harry|  Potter|1980-07-31|     M|100000000|\n",
      "|   Ronald|   Wemel|1980-04-01|     M|       10|\n",
      "+---------+--------+----------+------+---------+\n",
      "\n",
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|     F|       1|\n",
      "|     M|       2|\n",
      "+------+--------+\n",
      "\n",
      "+------+-----+\n",
      "|gender|count|\n",
      "+------+-----+\n",
      "|     F|    1|\n",
      "|     M|    2|\n",
      "+------+-----+\n",
      "\n",
      "+---------+--------+\n",
      "|firstname|lastname|\n",
      "+---------+--------+\n",
      "|    Harry|  Potter|\n",
      "|   Ronald|   Wemel|\n",
      "|Hermelijn| Griffel|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"test\")    # om een fictieve aan te maken waar je je sql-query op kan uitvoeren\n",
    "df_male = spark.sql(\"select * from test where gender='M'\")\n",
    "df_male.show()\n",
    "df_male2 = df.filter(df.gender == \"M\")\n",
    "df_male2.show()\n",
    "\n",
    "df_num_gender = spark.sql('select gender, count(*) from test group by gender')\n",
    "df_num_gender.show()\n",
    "df.groupby(\"gender\").count().show()\n",
    "\n",
    "df.select(\"firstname\", \"lastname\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Buiten de functionaliteit om SQL queries uit te voeren is ook het lezen en schrijven van allerhande dataformaten een belangrijk onderdeel van de pyspark sql module.\n",
    "Meer informatie hierover kun je [hier](https://spark.apache.org/docs/latest/sql-data-sources.html) vinden in de documentatie.\n",
    "In essentie ziet de code er uit als volgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De opties die hierbij gekozen kunnen worden kun je vinden in de documentatie.\n",
    "\n",
    "Daarnaast zijn er ook functionaliteiten om data uit te lezen speciaal voor Machine Learning zoals libsvm en image-directories maar die worden later getoond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Oefening\n",
    "\n",
    "Net zoals RDD kunnen er een aantal operaties uitgevoerd worden op deze dataframes.\n",
    "Een volledige lijst met alle operaties kan je [hier](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#dataframe-apis) vinden.\n",
    "Zoek de functies die gebruikt moeten worden om de volgende zaken uit te voeren:\n",
    "* Groepeer volgens een bepaalde sleutel\n",
    "* Krijg een lijst met alle kolomnamen\n",
    "* Filter rijen uit\n",
    "* Verwijder null-values in de dataset via rijen\n",
    "* Verwijder null-values door kolommen te verwijderen\n",
    "* Bereken een dataframe met statistieken van het dataframe\n",
    "* Krijg een dataframe met alle nan waarden\n",
    "* Hoe krijg je informatie zoals .info()\n",
    "* Hoe werkt het groeperen van informatie op basis van een key/kolom\n",
    "\n",
    "Probeer deze ook uit op bovenstaand aangemaakt dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lees daarna volgende [link](https://sparkbyexamples.com/pyspark/pyspark-aggregate-functions/) om een idee te krijgen over hoe verschillende functies uit te voeren op deze dataframes.\n",
    "Werk nu de volgende oefening uit en maak hiervoor een spark applicatie:\n",
    "* Download de iris dataset in sklearn\n",
    "* Bewaar deze dataset als csv en upload de file naar het hdfs\n",
    "* Schrijf de code om de csv uit te lezen en om te zetten naar een dataframe\n",
    "* Print het dataschema uit voor het dataframe, hoeveel kolommen zijn er aanwezig in het dataframe.\n",
    "* Bereken het minimum en maximum van de 'sepal width (cm)' en 'petal width (cm)' kolom.\n",
    "* Hernoem de target kolom naar label\n",
    "* Hernoem de labels 0 naar Soort 0, labels 1 naar Soort 2 en labels 3 naar Soort 3\n",
    "* Voer normalisatie van de eerste 4 kolommen uit (het gemiddelde ervan aftrekken en delen door de standaardafwijking)\n",
    "* Controleer de voorgaande stap door opnieuw het gemiddelde en de standaardafwijking te berekenen. Deze moeten respectievelijk 0 en 1 zijn.\n",
    "* Bereken de oppervlakte van sepal door de lengte en breedte ervan te vermenigvuldigen. Noem deze nieuwe kolom sepal area. Doe dit ook voor de petal.\n",
    "* Groepeer nu de rijen op de label kolom. Bereken per groep het gemiddelde van elke kolom. Is er een verschil tussen de verschillende klassen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shared variabelen**\n",
    "\n",
    "Variabelen met read-write acces zijn zeer inefficient om te gebruiken in een cluster met sterke parallelisatie.\n",
    "Spark bied echter twee varianten aan die wel efficient geimplementeerd kunnen worden, namelijk\n",
    "* Broadcasted variabelen\n",
    "* Accumulators\n",
    "\n",
    "Broadcasted variabelen zijn read-only variabelen, die aangemaakt worden door de driver en eenmalig verspreid worden over de nodes in plaats van voor elke job.\n",
    "Dit wordt vooral gebruikt om grote data die veelvuldig gebruikt wordt te cachen op de nodes.\n",
    "Bij het gebruik van broadcast variabelen is het belangrijk om te onthouden dat je de originele variabele niet meer mag gebruiken na het aanmaken van de broadcasted variabele omdat ze anders toch nog elke job doorgestuurd wordt.\n",
    "De belangrijkste functies om te werken met broadcasted variabelen zijn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accumulators**\n",
    "\n",
    "Het andere type dat aangeboden wordt zijn accumulators.\n",
    "Deze laten enkel toe dat noden iets toevoegen aan een gedeelde variabele.\n",
    "Enkel de driver kan deze variabele uitlezen.\n",
    "Dit kan bijvoorbeeld gebruikt worden om tellers of sommen bij te houden.\n",
    "Deze accumulators kunnen een naam hebben (named accumulators zijn zichtbaar in de wep api).\n",
    "De ingebouwde accumulator van Spark ondersteunt enkel numerieke accumulators.\n",
    "Het is echter mogelijk om eigen accumulators toe te voegen door over te erven van de AccumulatorParam klasse en deze twee functies te implementeren:\n",
    "* zero: De begin waarde van de accumulator\n",
    "* addInPlace: Om twee waarden samen te voegen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "Door de hoge populariteit van pandas in python is er een alternatief uitgewerkt binnen de laatste versie van Spark (eind 2021) dat de pandas api integreert.\n",
    "Hierdoor kan je code schrijven die identiek is aan te werken met pandas.\n",
    "Meer informatie over deze api vind je op de [pyspark documentatie](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/index.html).\n",
    "\n",
    "De volgende twee delen van deze documentatie zijn belangrijk:\n",
    "* [De beschikbare pandas functies](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/supported_pandas_api.html): Niet alle functies van pandas kunnen gebruikt worden. Dit komt doordat pandas geschreven is voor data die volledig lokaal ingeladen is in het geheugen. Omdat dit gevaarlijk kan zijn in het geval van grote datasets zijn de functies die dit zouden uitvoeren niet geimplementeerd. De meeste functies die we gebruiken zijn gelukkig wel geimplementeerd.\n",
    "* [De best practices](https://spark.apache.org/docs/latest/api/python/user_guide/pandas_on_spark/best_practices.html) om te werken met pandas on spark. Samengevat is het:\n",
    "  * Gebruik de .explain() functie in spark om het plan van uitvoering te bekijken indien het uitvoeren te lang duurt\n",
    "  * Gebruik checkpoints voor de efficientie van de planner van het stappenplan (spark.checkpoint() of spark.local_checkpoint()\n",
    "  * **Vermijd shuffling van data zoals sort-operaties**. Hierbij wordt data tussen nodes verstuurd wat kan resulteren in heel wat communicatie tussen verschillende nodes\n",
    "  * **Pas op met kolomnamen**: Gebruik geen duplicaten en vermijd gereserveerde namen met leading en trailing dubbele underscores\n",
    "  * **Stel waar mogelijk de index kolom in bij converteren naar een pandas-on-spark dataframe**: Veel functies werken efficienter bij het gebruik van een goede indexwaarde ipv de default.\n",
    "  * **Vermijd merge/join operaties waar mogelijk**: Ook deze operaties vereisen heel wat communicatie dus worden best vermeden\n",
    "  * Gebruik zoveel mogelijk van de pandas-on-spark API direct ipv standaard python functies om conflicten te vermijden\n",
    "     * df.sum() werkt gedistribueerd maar sum(df) niet wat tot fouten en errors leidt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verdere oefeningen\n",
    "\n",
    "Gebruik nu de titanic csv om met behulp van een spark applicatie de volgende zaken te berekenen:\n",
    "* Het aantal unieke plaatsen waar personen aan boord zijn gekomen (embarked kolom)\n",
    "* Het aantal ontbrekende waarden in de Cabin kolom\n",
    "* De volgende statistische waarden voor de ticketprijs (Fare) kolom: min, max, mean, std\n",
    "* De langste naam van een passagier\n",
    "* Het aantal passagiers per klasse\n",
    "* Het totaal aantal passagiers op de titanic\n",
    "\n",
    "Het is de bedoeling dat je elk element afzonderlijk berekend dus je moet geen dataframe uitkomen waar al deze zaken in zitten. \n",
    "\n",
    "Tip: herstart je kernel zodat je het opzetten van de sparkcontext ook oefent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
