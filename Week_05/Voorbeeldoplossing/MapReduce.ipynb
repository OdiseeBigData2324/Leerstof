{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6917b7e3",
   "metadata": {},
   "source": [
    "# Map Reduce Oefening - Schaken\n",
    "\n",
    "**Deadline:**\n",
    "\n",
    "In deze oefening gaan we werken met een dataset dat informatie bevat over een groot aantal schaakspelletjes.\n",
    "Deze dataset kan [hier](https://www.kaggle.com/datasnaek/chess) gevonden worden.\n",
    "Om te beginnen, download deze file en upload hem naar je distributed file systeem onder het path: **Oefingen/Mapreduce**.\n",
    "Voorzie ook code die deze folder reset naar een lege folder om geen naam-conflicten te hebben.\n",
    "Om je code te testen, print de eerste 1000 bytes uit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf9c85-fdb8-4494-be83-ed725509a5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download dataset\n",
    "import opendatasets as od\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/datasnaek/chess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cbff3dd-cc89-447e-9736-749a9d7e6942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,rated,created_at,last_move_at,turns,victory_status,winner,increment_code,white_id,white_rating,black_id,black_rating,moves,opening_eco,opening_name,opening_ply\n",
      "TZJHLljE,FALSE,1.50421E+12,1.50421E+12,13,outoftime,white,15+2,bourgris,1500,a-00,1191,d4 d5 c4 c6 cxd5 e6 dxe6 fxe6 Nf3 Bb4+ Nc3 Ba5 Bf4,D10,Slav Defense: Exchange Variation,5\n",
      "l1NXvwaE,TRUE,1.50413E+12,1.50413E+12,16,resign,black,5+10,a-00,1322,skinnerua,1261,d4 Nc6 e4 e5 f4 f6 dxe5 fxe5 fxe5 Nxe5 Qd4 Nc6 Qe5+ Nxe5 c4 Bb4+,B00,Nimzowitsch Defense: Kennedy Variation,4\n",
      "mIICvQHh,TRUE,1.50413E+12,1.50413E+12,61,mate,white,5+10,ischia,1496,a-00,1500,e4 e5 d3 d6 Be3 c6 Be2 b5 Nd2 a5 a4 c5 axb5 Nc6 bxc6 Ra6 Nc4 a4 c3 a3 Nxa3 Rxa3 Rxa3 c4 dxc4 d5 cxd5 Qxd5 exd5 Be6 Ra8+ Ke7 Bc5+ Kf6 Bxf8 Kg6 Bxg7 Kxg7 dxe6 Kh6 exf7 Nf6 Rxh8 Nh5 Bxh5 Kg5 Rxh7 Kf5 Qf3+ Ke6 Bg4+ Kd6 Rh6+ Kc5 Qe3+ Kb5 c4+ Kb4 Qc3+ Ka4 Bd1#,C20,King's Pawn Game: Leonardis Variation,3\n",
      "kWKvrqYL,TRUE,1.50411E+12,1.50411E+12,61,mate,white,20+0,daniamurashov,1439,adivanov\n"
     ]
    }
   ],
   "source": [
    "#upload csv file to hdfs\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "map = 'Oef_MapReduce'\n",
    "\n",
    "client = InsecureClient('http://localhost:9870', user='bigdata')\n",
    "\n",
    "if client.status(map, strict=False) is None:\n",
    "    client.makedirs(map)\n",
    "else:\n",
    "    # do some cleaning in case anything else than *.txt is present\n",
    "    for f in client.list(map):\n",
    "        if not f.endswith('games.csv'):\n",
    "            client.delete(map + '/' + f, recursive=True)\n",
    "            \n",
    "if client.status(map + '/games.csv', strict=False) is None:\n",
    "    client.upload(map, './chess/games.csv')\n",
    "\n",
    "with client.read(map + '/games.csv', length=1000) as reader:\n",
    "    content = reader.read()\n",
    "    print(content.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d052848a",
   "metadata": {},
   "source": [
    "# Map Reduce applicaties\n",
    "\n",
    "Nu dat de dataset geupload is kunnen we map-reduce applicaties schrijven om deze dataset te verwerken. Schrijf hieronder de nodige mapreduce applicaties om de elke oefening uit te voeren.\n",
    "Deze oefeningen bestaan steeds uit twee cellen:\n",
    "* Een eerste cel met de nodige python code voor de map-reduce applicatie. Sla deze code op in een file met het correcte oefeningennummer (bvb: oefening_1.py) door gebruik te maken van de \"%%file\" tag in de notebookcellen.\n",
    "* Een tweede cel die de map-reduce applicatie uitvoert op de cluster en het correcte bestand uitleest. Commando's uitvoeren naar de commandline/terminal gebeurt door een uitroepingsteken vooraan te plaatsen (!pydoop submit ...)\n",
    "\n",
    "**Maak gebruik van de mrjob package** in python en niet van de java code of andere aangehaalde libraries.\n",
    "\n",
    "## Dimensies\n",
    "\n",
    "De eerste gevraagde map-reduce applicatie telt het aantal rijen en het aantal kolommen.\n",
    "Schrijf deze nu hieronder en beantwoord de vragen na de code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23f528b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting oefening_1.py\n"
     ]
    }
   ],
   "source": [
    "%%file oefening_1.py\n",
    "from mrjob.job import MRJob\n",
    "\n",
    "class Vraag1(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        if line == \"id,rated,created_at,last_move_at,turns,victory_status,winner,increment_code,white_id,white_rating,black_id,black_rating,moves,opening_eco,opening_name,opening_ply\":\n",
    "            return\n",
    "\n",
    "        yield(\"row\", 1)\n",
    "        yield(\"columns\", line.count(\",\") + 1)\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        # dit is per key, je kan niet van de ene key aan data van een andere key\n",
    "\n",
    "        if key == \"row\":\n",
    "            yield(\"aantal rijen\", sum(counts))\n",
    "        elif key == \"columns\":\n",
    "            yield(\"aantal kolommen\", max(counts))\n",
    "        \n",
    "\n",
    "# de main functie komt hieronder\n",
    "if __name__ == \"__main__\":\n",
    "    Vraag1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04a528f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/oefening_1.root.20240319.074319.171897\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/oefening_1.root.20240319.074319.171897/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/oefening_1.root.20240319.074319.171897/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8129976839371676184/] [] /tmp/streamjob7734138006844556987.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n",
      "  Connecting to Application History server at historyserver/172.20.0.9:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n",
      "  Connecting to Application History server at historyserver/172.20.0.9:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1710832771836_0001\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1710832771836_0001\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1710832771836_0001\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1710832771836_0001/\n",
      "  Running job: job_1710832771836_0001\n",
      "  Job job_1710832771836_0001 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1710832771836_0001 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/oefening_1.root.20240319.074319.171897/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7676751\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=42\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1041\n",
      "\t\tFILE: Number of bytes written=853942\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7676969\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=42\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=33247232\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17981440\n",
      "\t\tTotal time spent by all map tasks (ms)=8117\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32468\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2195\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17560\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8117\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2195\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=3520\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=377\n",
      "\t\tInput split bytes=218\n",
      "\t\tMap input records=20059\n",
      "\t\tMap output bytes=421218\n",
      "\t\tMap output materialized bytes=1094\n",
      "\t\tMap output records=40116\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=260067328\n",
      "\t\tPeak Map Virtual memory (bytes)=5041655808\n",
      "\t\tPeak Reduce Physical memory (bytes)=268468224\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8386592768\n",
      "\t\tPhysical memory (bytes) snapshot=788033536\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce input records=40116\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=1094\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80232\n",
      "\t\tTotal committed heap usage (bytes)=549978112\n",
      "\t\tVirtual memory (bytes) snapshot=18468876288\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/oefening_1.root.20240319.074319.171897/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/oefening_1.root.20240319.074319.171897/output...\n",
      "\"aantal kolommen\"\t16\n",
      "\"aantal rijen\"\t20058\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/oefening_1.root.20240319.074319.171897...\n",
      "Removing temp directory /tmp/oefening_1.root.20240319.074319.171897...\n"
     ]
    }
   ],
   "source": [
    "# commando voor uitvoeren van de applicatie\n",
    "!python3 oefening_1.py -r hadoop hdfs:///user/bigdata/Oef_MapReduce/games.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676d5e3e",
   "metadata": {},
   "source": [
    "**Vragen:**\n",
    "\n",
    "- Hoeveel rijen heb je gevonden in de dataset?\n",
    "- Hoeveel spelletjes zitten er echt in de dataset (kijk hiervoor naar de link van de dataset)?\n",
    "- Zijn deze waarden gelijk? Indien nee, wat is de oorzaak van het verschil? Hoe zou je dit kunnen oplossen?\n",
    "\n",
    "**Antwoord:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255735fb",
   "metadata": {},
   "source": [
    "Schrijf hier je antwoord..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fde28b",
   "metadata": {},
   "source": [
    "Indien je waarden niet overeenkomen, maak een kopie van de vorige applicatie en pas het aan zodat het aantal spelletjes in de dataset correct geteld wordt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd489dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aangepast map-reduce applicatie om het correcte aantal te bekomen.\n",
    "%%file oefening1_gelijk.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1506006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commando voor uitvoeren van de applicatie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec360aa",
   "metadata": {},
   "source": [
    "## Verschil in rating\n",
    "\n",
    "Wat is het gemiddelde verschil in de rating tussen de beide spelers? Hoe vaak wint de sterkste speler (geef dit terug als percentage van het aantal geregistreerde spelletjes)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730d9ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting oefening_2.py\n"
     ]
    }
   ],
   "source": [
    "%%file oefening_2.py\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "col_winner = 6\n",
    "col_white_rating = 9\n",
    "col_black_rating = 11\n",
    "\n",
    "class Vraag2(MRJob):\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        if line == \"id,rated,created_at,last_move_at,turns,victory_status,winner,increment_code,white_id,white_rating,black_id,black_rating,moves,opening_eco,opening_name,opening_ply\":\n",
    "            return\n",
    "        \n",
    "        # line of text to list of columns\n",
    "        csv_file = StringIO(line)\n",
    "        cols = next(csv.reader(csv_file))  \n",
    "\n",
    "        speler1 = float(cols[col_white_rating])\n",
    "        speler2 = float(cols[col_black_rating])\n",
    "        winner = cols[col_winner]\n",
    "\n",
    "        yield(\"verschil\", abs(speler1-speler2))\n",
    "\n",
    "        if winner==\"white\" and speler1 >= speler2:\n",
    "            yield(\"sterkste\", 1)\n",
    "        elif winner==\"black\" and speler2 >= speler1:\n",
    "            yield(\"sterkste\", 1)\n",
    "        else:\n",
    "            yield(\"sterkste\", 0)\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        # dit is per key maar beide keys kunnen op dezelfde manier verwerkt worden\n",
    "\n",
    "        counts = list(counts)\n",
    "\n",
    "        yield(key, sum(counts)/ len(counts))\n",
    "        \n",
    "\n",
    "# de main functie komt hieronder\n",
    "if __name__ == \"__main__\":\n",
    "    Vraag2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1062bd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/oefening_2.root.20240319.075302.351123\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/oefening_2.root.20240319.075302.351123/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/oefening_2.root.20240319.075302.351123/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar6283131116140561786/] [] /tmp/streamjob2554835464661704698.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n",
      "  Connecting to Application History server at historyserver/172.20.0.9:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n",
      "  Connecting to Application History server at historyserver/172.20.0.9:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1710832771836_0002\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1710832771836_0002\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1710832771836_0002\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1710832771836_0002/\n",
      "  Running job: job_1710832771836_0002\n",
      "  Job job_1710832771836_0002 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1710832771836_0002 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/oefening_2.root.20240319.075302.351123/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7676751\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=58\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=48643\n",
      "\t\tFILE: Number of bytes written=949654\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7676969\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=58\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=53059584\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=17743872\n",
      "\t\tTotal time spent by all map tasks (ms)=12954\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=51816\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2166\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17328\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=12954\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2166\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7280\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=332\n",
      "\t\tInput split bytes=218\n",
      "\t\tMap input records=20059\n",
      "\t\tMap output bytes=591381\n",
      "\t\tMap output materialized bytes=49201\n",
      "\t\tMap output records=40116\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=311083008\n",
      "\t\tPeak Map Virtual memory (bytes)=5041942528\n",
      "\t\tPeak Reduce Physical memory (bytes)=267608064\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8387670016\n",
      "\t\tPhysical memory (bytes) snapshot=887910400\n",
      "\t\tReduce input groups=2\n",
      "\t\tReduce input records=40116\n",
      "\t\tReduce output records=2\n",
      "\t\tReduce shuffle bytes=49201\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=80232\n",
      "\t\tTotal committed heap usage (bytes)=735051776\n",
      "\t\tVirtual memory (bytes) snapshot=18470100992\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/oefening_2.root.20240319.075302.351123/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/oefening_2.root.20240319.075302.351123/output...\n",
      "\"sterkste\"\t0.6252866686608834\n",
      "\"verschil\"\t173.091434838967\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/oefening_2.root.20240319.075302.351123...\n",
      "Removing temp directory /tmp/oefening_2.root.20240319.075302.351123...\n"
     ]
    }
   ],
   "source": [
    "# commando voor uitvoeren van de applicatie\n",
    "!python3 oefening_2.py -r hadoop hdfs:///user/bigdata/Oef_MapReduce/games.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bdbcf4",
   "metadata": {},
   "source": [
    "## Uitkomsten per kleur\n",
    "\n",
    "Schrijf nu een map-reduce applicatie dat telt welke uitkomst (status) het vaakst leidt tot een winnaar van de witte of zwarte kleur.\n",
    "Als eindresultaat wil ik dus bijvoorbeeld het volgende zien (kleuren zijn niet noodzakelijk correct):\n",
    "* mate          white\n",
    "* outoftime     black\n",
    "* draw          black\n",
    "* resign        white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4263e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file oefening_3.py\n",
    "# uitkomsten per kleur\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "col_victory_status = 5\n",
    "col_winner = 6\n",
    "col_white_rating = 9\n",
    "col_black_rating = 11\n",
    "\n",
    "class Vraag3(MRJob):\n",
    "\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        if line == \"id,rated,created_at,last_move_at,turns,victory_status,winner,increment_code,white_id,white_rating,black_id,black_rating,moves,opening_eco,opening_name,opening_ply\":\n",
    "            return\n",
    "\n",
    "        # line of text to list of columns\n",
    "        csv_file = StringIO(line)\n",
    "        cols = next(csv.reader(csv_file))    \n",
    "\n",
    "        yield (cols[col_victory_status], cols[col_winner])\n",
    "        \n",
    "    def reducer(self, word, counts):\n",
    "\n",
    "        counts = list(counts)\n",
    "\n",
    "        count_white = counts.count('white')\n",
    "        count_black = counts.count('black')\n",
    "\n",
    "        yield (word, 'white' if count_white >= count_black else 'black')\n",
    "        \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Vraag3.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6570f47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commando voor uitvoeren van de applicatie\n",
    "!python3 oefening_3.py -r hadoop hdfs:///user/bigdata/Oef_MapReduce/games.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c62fde3",
   "metadata": {},
   "source": [
    "## Aantal keer schaak per spelletje\n",
    "\n",
    "Schaak tel je door een '+' teken in de moves kolom. Schaakmat door een '#' teken. Bereken de volgende statistieken van het aantal keer dat er schaak (schaakmat tel je ook mee) is per schaakspelletje:\n",
    "* Minimum\n",
    "* Maximum\n",
    "* Gemiddelde\n",
    "* Standaardafwijking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c0368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing oefening_4.py\n"
     ]
    }
   ],
   "source": [
    "%%file oefening_4.py\n",
    "# aantal keer shcaak\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "col_victory_status = 5\n",
    "col_winner = 6\n",
    "col_white_rating = 9\n",
    "col_black_rating = 11\n",
    "col_moves = 12\n",
    "\n",
    "class Vraag4(MRJob):\n",
    "\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        if line == \"id,rated,created_at,last_move_at,turns,victory_status,winner,increment_code,white_id,white_rating,black_id,black_rating,moves,opening_eco,opening_name,opening_ply\":\n",
    "            return\n",
    "\n",
    "        # line of text to list of columns\n",
    "        csv_file = StringIO(line)\n",
    "        cols = next(csv.reader(csv_file))    \n",
    "\n",
    "        aantal_keer_schaak = cols[col_moves].count('+')\n",
    "        aantal_keer_mat = cols[col_moves].count('#')\n",
    "        \n",
    "        yield (\"aantal schaak\", aantal_keer_mat + aantal_keer_schaak)\n",
    "        \n",
    "    def reducer(self, word, counts):\n",
    "\n",
    "        counts = list(counts)\n",
    "\n",
    "        yield(\"min\", min(counts))\n",
    "        yield(\"max\", max(counts))\n",
    "        \n",
    "        yield(\"mean\", sum(counts) / len(counts))\n",
    "        yield(\"std\", np.std(counts))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Vraag4.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ebf4136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/oefening_4.root.20240319.080001.741313\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/oefening_4.root.20240319.080001.741313/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/oefening_4.root.20240319.080001.741313/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar2313002637818764879/] [] /tmp/streamjob8416503118722809363.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n",
      "  Connecting to Application History server at historyserver/172.20.0.9:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n",
      "  Connecting to Application History server at historyserver/172.20.0.9:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1710832771836_0003\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1710832771836_0003\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1710832771836_0003\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1710832771836_0003/\n",
      "  Running job: job_1710832771836_0003\n",
      "  Job job_1710832771836_0003 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1710832771836_0003 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/oefening_4.root.20240319.080001.741313/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7676751\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=66\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20325\n",
      "\t\tFILE: Number of bytes written=892724\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7676969\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=66\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=32874496\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=16269312\n",
      "\t\tTotal time spent by all map tasks (ms)=8026\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=32104\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1986\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=15888\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=8026\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=1986\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2720\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=334\n",
      "\t\tInput split bytes=218\n",
      "\t\tMap input records=20059\n",
      "\t\tMap output bytes=364175\n",
      "\t\tMap output materialized bytes=20589\n",
      "\t\tMap output records=20058\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=270438400\n",
      "\t\tPeak Map Virtual memory (bytes)=5042794496\n",
      "\t\tPeak Reduce Physical memory (bytes)=270831616\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8388091904\n",
      "\t\tPhysical memory (bytes) snapshot=807763968\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=20058\n",
      "\t\tReduce output records=4\n",
      "\t\tReduce shuffle bytes=20589\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=40116\n",
      "\t\tTotal committed heap usage (bytes)=548929536\n",
      "\t\tVirtual memory (bytes) snapshot=18473287680\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/oefening_4.root.20240319.080001.741313/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/oefening_4.root.20240319.080001.741313/output...\n",
      "\"min\"\t0\n",
      "\"max\"\t51\n",
      "\"mean\"\t5.069099611127729\n",
      "\"std\"\t5.063630552210335\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/oefening_4.root.20240319.080001.741313...\n",
      "Removing temp directory /tmp/oefening_4.root.20240319.080001.741313...\n"
     ]
    }
   ],
   "source": [
    "# commando voor uitvoeren van de applicatie\n",
    "!python3 oefening_4.py -r hadoop hdfs:///user/bigdata/Oef_MapReduce/games.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805e81c",
   "metadata": {},
   "source": [
    "## Verificatie van de kolom met het aantal zetten\n",
    "\n",
    "In de dataset zijn twee manieren aanwezig om het aantal zetten in een schaakspel te bekomen.\n",
    "* Er is een turns kolom dat het totaal aantal beurten vast heeft\n",
    "* Er is ook een moves kolom dat alle uitgevoerde zetten bevat (gescheidden door een spatie)\n",
    "\n",
    "Kijk of deze waarden voor alle rijen overeenkomt en tel de volgende waarden:\n",
    "* Totaal aantal schaakspelletjes\n",
    "* Aantal waar de kolommen overeenkomen\n",
    "* Aantal waar de kolommen niet overeenkomen.\n",
    "\n",
    "Zorg ervoor dat deze drie waarden aanwezig zijn in het uiteindelijke resultaat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea35727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file oefening_5.py\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "col_turns = 4\n",
    "col_victory_status = 5\n",
    "col_winner = 6\n",
    "col_white_rating = 9\n",
    "col_black_rating = 11\n",
    "col_moves = 12\n",
    "\n",
    "class Vraag5(MRJob):\n",
    "\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        if line == \"id,rated,created_at,last_move_at,turns,victory_status,winner,increment_code,white_id,white_rating,black_id,black_rating,moves,opening_eco,opening_name,opening_ply\":\n",
    "            return\n",
    "\n",
    "        # line of text to list of columns\n",
    "        csv_file = StringIO(line)\n",
    "        cols = next(csv.reader(csv_file)) \n",
    "\n",
    "        turns = int(cols[col_turns])\n",
    "        moves = cols[col_moves].count(' ') + 1 # plus 1 voor de eerste zet\n",
    "\n",
    "        yield (\"aantal zetten matches\", 1 if turns == moves else 0)\n",
    "        \n",
    "    def reducer(self, word, counts):\n",
    "\n",
    "        counts = list(counts)\n",
    "\n",
    "        yield ('aantal', len(counts))\n",
    "        yield ('overeenkomst', sum(counts))\n",
    "        yield ('niet-overeenkomst', len(counts) - sum (counts))\n",
    "        \n",
    "\n",
    "if __name__ == '__main__'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa49e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commando voor uitvoeren van de applicatie\n",
    "!python3 oefening_5.py -r hadoop hdfs:///user/bigdata/Oef_MapReduce/games.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaab5715",
   "metadata": {},
   "source": [
    "## De verschillende openingen\n",
    "\n",
    "Schrijf nu een map-reduce applicatie dat het aantal verschillende openingen in de dataset telt. Tel daarnaast ook hoe vaak elke opening voorkomt. Gebruik hiervoor de naam van de openingen. Tip: een set in python bevat geen duplicaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799bd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file oefening_6.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "\n",
    "col_turns = 4\n",
    "col_victory_status = 5\n",
    "col_winner = 6\n",
    "col_white_rating = 9\n",
    "col_black_rating = 11\n",
    "col_moves = 12\n",
    "col_openingen = 14\n",
    "\n",
    "class Vraag6(MRJob):\n",
    "    def mapper(self,_,line):\n",
    "\n",
    "        if line == \"id,rated,created_at,last_move_at,turns,victory_status,winner,increment_code,white_id,white_rating,black_id,black_rating,moves,opening_eco,opening_name,opening_ply\":\n",
    "            return\n",
    "\n",
    "        # line of text to list of columns\n",
    "        csv_file = StringIO(line)\n",
    "        cols = next(csv.reader(csv_file)) \n",
    "        \n",
    "        yield(\"opening\", cols[col_openingen])\n",
    "        \n",
    "        # aantal keer dat elke opening voorkomt\n",
    "        #yield(cols[col_openingen], 1)\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "        \n",
    "        counts = list(counts)\n",
    "        unieke = set(counts)\n",
    "\n",
    "        yield(\"aantal unieke\", len(unieke))\n",
    "\n",
    "        # manier 2 voor het aantal openingen te tellen\n",
    "        for opening in unieke:\n",
    "            yield(opening, counts.count(opening))\n",
    "        \n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    Vraag6.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7b0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commando voor uitvoeren van de applicatie\n",
    "!python3 oefening_6.py -r hadoop hdfs:///user/bigdata/Oef_MapReduce/games.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3325a0",
   "metadata": {},
   "source": [
    "## verscheidene statistieken in 1 applicatie\n",
    "\n",
    "Maak nu een map-reduce applicatie dat de volgende zaken in 1 keer berekend:\n",
    "* Het minimum, maximum en gemiddelde aantal minuten dat een schaakspel geduurd heeft (de tijden aanwezig in de dataset zijn in [Unix-format](https://en.wikipedia.org/wiki/Unix_time))\n",
    "* Een histogram van het aantal zetten in een schaakspel. Plaats de grensen van de bins in de naam van de resultaat op de volgende manier: bin_{ondergrens}_{bovengrens}. Gebruik hierbij 10 bins die het bereik van 1-51 verdelen (0 zetten worden dus niet geteld).\n",
    "* De openingsstrategie die het vaakst leidt tot winst voor de witte speler. Tip: collections package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebdb7165-113a-400a-a01b-acff743b02fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting oefening_7.py\n"
     ]
    }
   ],
   "source": [
    "%%file oefening_7.py\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "import csv\n",
    "from io import StringIO\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "col_start = 2\n",
    "col_last_move = 3\n",
    "col_turns = 4\n",
    "col_victory_status = 5\n",
    "col_winner = 6\n",
    "col_white_rating = 9\n",
    "col_black_rating = 11\n",
    "col_moves = 12\n",
    "col_openingen = 14\n",
    "\n",
    "class Vraag7(MRJob):\n",
    "\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "\n",
    "        if line == \"id,rated,created_at,last_move_at,turns,victory_status,winner,increment_code,white_id,white_rating,black_id,black_rating,moves,opening_eco,opening_name,opening_ply\":\n",
    "            return\n",
    "\n",
    "        # line of text to list of columns\n",
    "        csv_file = StringIO(line)\n",
    "        cols = next(csv.reader(csv_file)) \n",
    "\n",
    "        last_move = float(cols[col_last_move])\n",
    "        first_move = float(cols[col_start])\n",
    "        yield(\"aantal minuten\", (last_move - first_move) / 1000 / 60)\n",
    "\n",
    "        aantal_zetten = int(cols[col_turns])\n",
    "        if aantal_zetten > 0:\n",
    "            yield(\"aantal zetten\", aantal_zetten)\n",
    "\n",
    "        if cols[col_winner] == 'white':\n",
    "            yield(\"opening\", cols[col_openingen])\n",
    "\n",
    "    def reducer(self, key, counts):\n",
    "\n",
    "        counts = list(counts)\n",
    "\n",
    "        if key == \"aantal minuten\":\n",
    "            yield('min', min(counts))\n",
    "            yield('max', max(counts))\n",
    "            yield('mean', sum(counts)/len(counts))\n",
    "        elif key == 'opening':\n",
    "            counts = Counter(counts)\n",
    "            yield('beste strategie', counts.most_common(1)[0])\n",
    "        elif key == 'aantal zetten':\n",
    "            bin_limits = [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51]\n",
    "            hist, bin_edges = np.histogram(counts, bins=bin_limits)\n",
    "\n",
    "            for i in range(len(bin_edges) - 1):\n",
    "                yield(f\"bin_{bin_edges[i]}_{bin_edges[i+1]}\", int(hist[i]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    Vraag7.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "307e32cc-be53-4a14-8d7d-17480bb2ee2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "No configs specified for hadoop runner\n",
      "Looking for hadoop binary in /opt/hadoop-3.3.6/bin...\n",
      "Found hadoop binary: /opt/hadoop-3.3.6/bin/hadoop\n",
      "Using Hadoop version 3.3.6\n",
      "Looking for Hadoop streaming jar in /opt/hadoop-3.3.6...\n",
      "Found Hadoop streaming jar: /opt/hadoop-3.3.6/share/hadoop/tools/lib/hadoop-streaming-3.3.6.jar\n",
      "Creating temp directory /tmp/oefening_7.root.20240319.082020.760613\n",
      "uploading working dir files to hdfs:///user/root/tmp/mrjob/oefening_7.root.20240319.082020.760613/files/wd...\n",
      "Copying other local files to hdfs:///user/root/tmp/mrjob/oefening_7.root.20240319.082020.760613/files/\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [/tmp/hadoop-unjar8063382338649587036/] [] /tmp/streamjob7534928226200867496.jar tmpDir=null\n",
      "  Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n",
      "  Connecting to Application History server at historyserver/172.20.0.9:10200\n",
      "  Connecting to ResourceManager at resourcemanager/172.20.0.7:8032\n",
      "  Connecting to Application History server at historyserver/172.20.0.9:10200\n",
      "  Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1710832771836_0005\n",
      "  Total input files to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1710832771836_0005\n",
      "  Executing with tokens: []\n",
      "  resource-types.xml not found\n",
      "  Unable to find 'resource-types.xml'.\n",
      "  Submitted application application_1710832771836_0005\n",
      "  The url to track the job: http://resourcemanager:8088/proxy/application_1710832771836_0005/\n",
      "  Running job: job_1710832771836_0005\n",
      "  Job job_1710832771836_0005 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 50% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1710832771836_0005 completed successfully\n",
      "  Output directory: hdfs:///user/root/tmp/mrjob/oefening_7.root.20240319.082020.760613/output\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=7676751\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=295\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=154605\n",
      "\t\tFILE: Number of bytes written=1163554\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=7676969\n",
      "\t\tHDFS: Number of bytes read erasure-coded=0\n",
      "\t\tHDFS: Number of bytes written=295\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=11\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-milliseconds taken by all map tasks=39145472\n",
      "\t\tTotal megabyte-milliseconds taken by all reduce tasks=18251776\n",
      "\t\tTotal time spent by all map tasks (ms)=9557\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=38228\n",
      "\t\tTotal time spent by all reduce tasks (ms)=2228\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=17824\n",
      "\t\tTotal vcore-milliseconds taken by all map tasks=9557\n",
      "\t\tTotal vcore-milliseconds taken by all reduce tasks=2228\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=4960\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1228\n",
      "\t\tInput split bytes=218\n",
      "\t\tMap input records=20059\n",
      "\t\tMap output bytes=1401260\n",
      "\t\tMap output materialized bytes=157139\n",
      "\t\tMap output records=50117\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPeak Map Physical memory (bytes)=262443008\n",
      "\t\tPeak Map Virtual memory (bytes)=5040078848\n",
      "\t\tPeak Reduce Physical memory (bytes)=271380480\n",
      "\t\tPeak Reduce Virtual memory (bytes)=8386928640\n",
      "\t\tPhysical memory (bytes) snapshot=791810048\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=50117\n",
      "\t\tReduce output records=14\n",
      "\t\tReduce shuffle bytes=157139\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=100234\n",
      "\t\tTotal committed heap usage (bytes)=541065216\n",
      "\t\tVirtual memory (bytes) snapshot=18466566144\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "job output is in hdfs:///user/root/tmp/mrjob/oefening_7.root.20240319.082020.760613/output\n",
      "Streaming final output from hdfs:///user/root/tmp/mrjob/oefening_7.root.20240319.082020.760613/output...\n",
      "\"min\"\t0.0\n",
      "\"max\"\t10097.411683333334\n",
      "\"mean\"\t14.495117493435746\n",
      "\"bin_1_6\"\t382\n",
      "\"bin_6_11\"\t299\n",
      "\"bin_11_16\"\t480\n",
      "\"bin_16_21\"\t634\n",
      "\"bin_21_26\"\t786\n",
      "\"bin_26_31\"\t985\n",
      "\"bin_31_36\"\t1113\n",
      "\"bin_36_41\"\t1264\n",
      "\"bin_41_46\"\t1368\n",
      "\"bin_46_51\"\t1639\n",
      "\"beste strategie\"\t[\"Scandinavian Defense: Mieses-Kotroc Variation\", 164]\n",
      "Removing HDFS temp directory hdfs:///user/root/tmp/mrjob/oefening_7.root.20240319.082020.760613...\n",
      "Removing temp directory /tmp/oefening_7.root.20240319.082020.760613...\n"
     ]
    }
   ],
   "source": [
    "# commando voor uitvoeren van de applicatie\n",
    "!python3 oefening_7.py -r hadoop hdfs:///user/bigdata/Oef_MapReduce/games.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0426f218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "d5e8e3a19af5ceb2434683dff87da6345c3b29f7eb0a8a138558c07d014a01cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
